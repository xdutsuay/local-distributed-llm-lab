# Component Detail Documentation

This document provides deep technical details on the core components of LLM Lab. It is intended for developers who need to understand the internal logic, state management, and control flow.

## 1. Coordinator (`coordinator/main.py`)

The Coordinator is the implementation of the central server. It is a FastAPI application that serves as the gateway for all external interactions.

### Startup Lifecycle (`startup_event`)
When the application starts:
1.  **Ray Initialization**: Connects to the local Ray cluster using namespace `llm-lab`. Configured to ignore re-init errors for development reliability.
2.  **Tool Registry**: Calls `tools.registry.get_tool_registry()` to load available tools.
3.  **Node Registry**: Starts the `NodeRegistry` which begins listening for heartbeats.
4.  **Self-Registration**: Starts a background task `coordinator_heartbeat_loop` that periodically publishes its presence to the message bus, allowing it to appear on the dashboard as a node.

### API Endpoints

| Method | Endpoint | Purpose |
| :--- | :--- | :--- |
| `POST` | `/chat` | Main entry point for inference. Accepts `ChatRequest` (prompt, model). Triggers `WorkflowManager.invoke()`. |
| `GET` | `/api/nodes` | Returns JSON list of active nodes from `NodeRegistry`. |
| `GET` | `/api/tasks` | Returns recent task history (in-memory list). |
| `GET` | `/api/tools` | Lists registered tools and their usage stats. |
| `GET` | `/chat_ui` | Serves the Chat Interface HTML (`frontend/chat.html`). |
| `WS` | `/ws/join` | WebSocket for mobile/external nodes to stream heartbeats. |

### Websocket Protocol (`/ws/join`)
Used by mobile nodes or external scripts to join the cluster.
- **Message Format**: JSON object mimicking the internal heartbeat structure.
- **Translation**: Received messages are injected into the internal `RayMessageBus` on the `HEARTBEAT` topic.

---

## 2. Workflow & Planner (`coordinator/graph.py`)

LLM Lab uses **LangGraph** to model the execution flow as a state machine.

### State Schema (`AgentState`)
- `user_query`: Original prompt.
- `plan`: List of steps generated by the Planner.
- `results`: List of outputs (agent scratchpad).
- `execution_trace`: Telemetry data for each step.

### Execution Flow
1.  **Planner Node**:
    - Uses `coordinator.planner.Planner` (LLM-based) to break query into steps.
    - Example Plan: `[{"step": 1, "desc": "Search web"}, {"step": 2, "desc": "Summarize"}]`
2.  **Executor Node**:
    - Iterates through the plan `current_step_index`.
    - **Resolution**: Determines if the step needs a Tool or pure Generation.
    - **Dispatch**: Calls `WorkerPool.get_next_worker()`.
    - **Execution**: Invokes `worker.generate.remote(prompt)`.
    - **Tracing**: Records duration and responsible node ID.

---

## 3. Worker System (`coordinator/worker.py`)

Workers are Ray Actors that perform the heavy lifting. They are resilient and self-describing.

### Initialization & Auto-Detection
On `__init__`:
1.  **Model Check**: Runs `ollama list` subprocess to find available models.
2.  **Fallback**: If no model specified in env vars, uses the first detected model (e.g., `qwen2.5-coder` or `llama3.2`).
3.  **Heartbeat**: Starts a background loop publishing status to Ray PubSub.

### The `generate(prompt)` Method
The core inference logic with built-in efficiency layers:

1.  **Cache Lookup**:
    - Hashes the prompt + model name.
    - Queries `CacheManager`.
    - **IF HIT**: Returns cached JSON immediately (skips inference).
2.  **External API (Optional)**:
    - If `api_base` is set (e.g., LM Studio), sends HTTP POST.
3.  **Local Inference**:
    - Calls `ollama.chat()`.
4.  **Cache Store**:
    - Saves successful result to `CacheManager`.
    - Updates local `generated_bytes` metric.

---

## 4. Distributed Caching (`coordinator/cache_manager.py`)

A singleton manager handling the "Short-term Memory" of the cluster to reduce costs and latency.

### Key Features
- **Storage**: Currently uses Ray's Object Store (via Python dictionary in actor memory for Phase 11, moving to distributed objects in Phase 12).
- **TTL (Time-To-Live)**: Default 1 hour. Stale entries are returned as `None`.
- **Key Generation**:
    ```python
    hash = md5(f"{model}:{prompt}:{temperature}".encode()).hexdigest()
    ```

---

## 5. Resiliency Patterns

### Round-Robin Load Balancing (`coordinator/worker_pool.py`)
- Maintains a list of `RayActorHandles`.
- `get_next_worker()` uses a mutex-protected counter to cycle through workers.
- Ensures even distribution of inference tasks across the cluster.

### Heartbeat System
- **Nodes** (Workers, Mobile, Coordinator) publish heartbeats every 5s.
- **Registry** subscribes to the channel.
- **TTL**: If a node is silent for 15s (3 missed beats), it is marked "Offline".
- **Self-Healing**: Workers that crash are restarted by Ray/Supervisor (configured in `supervisord.conf` or Ray specs), but currently rely on script restart.

### Error Handling in Workflow
- If a worker fails, `Executor Node` catches the exception.
- Records `error` in the trace.
- Does **not** crash the entire workflow; returns partial results or error message to user.
