# Local Distributed LLM Lab

**A distributed cognition framework for local AI experiments.**

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python](https://img.shields.io/badge/python-3.12.2-blue.svg)
![Status](https://img.shields.io/badge/status-active-green.svg)
![Tests](https://img.shields.io/badge/tests-27%2F43%20passing-green.svg)

## ğŸš€ Overview
**Local Distributed LLM Lab** orchestrates multiple local devicesâ€”laptops, desktops, and mobile phonesâ€”into a single collaborative AI cluster. Instead of sharding model weights, it focuses on **task-level parallelism** and **heterogeneous agents**.

A Planner LLM decomposes complex queries into subtasks, which are routed to the most appropriate worker node.

## âœ¨ Current Features

### Core Capabilities
- âœ… **Distributed Coordination** - Powered by Ray and FastAPI
- âœ… **Multi-Node Support** - macOS â†” Windows cross-platform
- âœ… **Task Attribution** - Track which node processed each task
- âœ… **Heartbeat System** - Auto-registration and TTL expiration
- âœ… **Agentic Workflow** - LangGraph task decomposition
- âœ… **Test Coverage** - 43 tests (27 passing, 16 future stubs)

### Interfaces
- ğŸ“Š **Dashboard** (`/llmlab`) - Real-time cluster status
- ğŸ’¬ **Chat UI** (`/chat_ui`) - Query interface
- ğŸ“‹ **Shared Clipboard** (`/memo`) - Cross-machine text transfer

## ğŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Coordinator Node (Mac)              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ FastAPI  â”‚  â”‚ LangGraphâ”‚  â”‚Ray Head   â”‚ â”‚
â”‚  â”‚ Planner  â”‚  â”‚ Workflow â”‚  â”‚Registry   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                    â”‚
    Heartbeats (5s)      Task Distribution
           â”‚                    â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
    â”‚               â”‚                  â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚Worker 1â”‚    â”‚ Worker 2 â”‚    â”‚ Mobile PWA â”‚
â”‚(Ollama)â”‚    â”‚ (Ollama) â”‚    â”‚ (planned)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ›  Quick Start

### Prerequisites
- Python 3.12.2
- [Ollama](https://ollama.com/) running locally
- Ray 2.53.0+

### Installation
```bash
git clone https://github.com/xdutsuay/local-distributed-llm-lab.git
cd local-distributed-llm-lab
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### Start Coordinator (Machine 1)
```bash
./scripts/start_coordinator.sh
# Dashboard: http://localhost:8000/llmlab
```

### Connect Worker (Machine 2)
```bash
./scripts/start_worker.sh <COORDINATOR_IP>
```

### Health Check
```bash
python scripts/health_check.py
```

## ğŸ§ª Testing
```bash
# All tests
python -m pytest tests/

# Active tests only
python -m pytest tests/ -k "not skip"
```

## ğŸ“ Project Structure
```
LLMLAB/
â”œâ”€â”€ coordinator/      # Core orchestration (main, graph, worker, registry)
â”œâ”€â”€ frontend/         # HTML/JS interfaces  
â”œâ”€â”€ tests/           # 43 tests (routes, heartbeat, cluster, etc.)
â”œâ”€â”€ scripts/         # Startup & diagnostic utilities
â”œâ”€â”€ config/          # Configuration files
â”œâ”€â”€ docs/            # Documentation
â””â”€â”€ archive/         # Historical files
```

## ğŸ—º Roadmap

### Completed
- [x] Multi-node distributed execution
- [x] LangGraph task orchestration
- [x] Node registry & health monitoring
- [x] Task attribution & composition
- [x] Comprehensive test suite
- [x] **Phase 10**: Prompt passing fix, round-robin load balancing, auto-detect Ollama model

### In Progress (Phase 11)
- [ ] Mobile mesh integration
- [ ] Tool execution framework
- [ ] Distributed caching & replication

### Planned
- [ ] **Phase 12**: Observability (timeline, metrics dashboard)
- [ ] **Phase 13**: Advanced scheduling strategies

## ğŸ”§ Configuration

### Environment Variables
```bash
export OLLAMA_MODEL=llama3.2  # Or mistral, gemma:2b, etc.
export RAY_ENABLE_WINDOWS_OR_OSX_CLUSTER=1
```

### Ray Namespace
All nodes must connect to namespace: `llm-lab`

## ğŸ¤ Contributing
Contributions welcome! See active issues and test coverage in `tests/`.

## ğŸ“œ License
MIT License.
